# Deloitte_Machinehack_Loan_Default_prediction
This repo is about the time when I participated in a DS challenge which was held on Machinehack . This project is about predicting Loan Default. 
As I said earlier this is a Loan Default Prediction problem. The dataset provided had a TRAIN csv file, TEST csv file and a Sample submission file. 
The train file had 67000 rows and 35 columns. This means that it had 34 features and 1 label and thw label being the customer defaulted or not. 
For this challenge the end goal was to have a low log_loss error. 
Log loss error is calculated on the predicted probabilities of the default or not (0 or1). This is done using predict_proba function in the appropriate function.
So, of the 35 features the dataset had, 21 were numerical and rest 14 were categorical. 
There was not much correlation between the features and the label. Dataset was fairly clean and had no missing values apart from features not having any values.
Data cleaning and preprocessing part was fairly easy. But the problem was: the dataset was imbalanced and something had to be done about that. 
Coming to modelling and evaluation I tried with the basic algorithms like Logistic, SVM, SGD, NAIVE BAYES and others and the results were bad.(log loss scores over 6).
Then I started with trees and ensemble algorithms. Decision tree was fairly poor and gradient bossting algorithms showed some promise. Then I moved towards xgboost, LGBM and Catboost Algorithms. 
Catboost showed the most promise with 0.4 log loss. With 0.4 as log loss error it was fairly good but it was still ot good. Then I tried to use sampling techniques using IMBLEARN library. I used various techniques like SMOTE, SMOTEEN, SMOTETOMeK. SMOTETOMeK showed the best results with 0.25 log loss levels. After that I tried on various hyperparameter combinations to further reduce the log loss to 0.199 levels. 
Train and valid scores were around 0.199 but when I submitted the predictions on the platform it showed 0.505. This was maybe due to overfitting problem. It was overfitting before too then I created some new features. Some were the basic stuff like adding mean and standard deviation values of the features, adding, subtracting and multiplying some realted features that were best to my knowledge. Then I did some complex feature engineering by using using different weights for different features to create a new feature. My motivation was using the method that was used by credit rating agencies. But I found low success. First there was hadly any correlation between the new feature(credit_score) and default and it also showed poor effect in feature importance. So, the new features were didn't do the job.
Then I turned to neural networks. I am a complete novice at Deep Learning. I just have the basic knowledge of neaural networks and tensorflow and keras. So to save time I had earlier collected a ipynb file of a deep learning model from the interent. So I just did just minor adjustments like data loading, data preprocessing and added some extra layers of neurons and set the right concrened metric (log_loss) and I trained the model. To my surprise it just had 0.303 log_loss. Also, when I submitted to the competition the score was 0.35. So, it wasn't overfitting as I had previously faced with Catboost. 
